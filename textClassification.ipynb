{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f156dd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#подключение библиотек и модулей\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from pymorphy3 import MorphAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbc71086",
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция извлечения текста из заданного URL-адреса.\n",
    "def get_text_from_url(URL):\n",
    "    html = urlopen(URL).read()\n",
    "    soup = BeautifulSoup(html, 'html')\n",
    "    return soup.get_text().replace('--','-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f07da1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#загрузка текстов разных авторов\n",
    "shol = get_text_from_url('http://www.lib.ru/PROZA/SHOLOHOW/tihijdon34.txt')\n",
    "klsh= get_text_from_url('http://www.lib.ru/HIST/KALASHNIKOW/zv_b1.txt')\n",
    "chern = get_text_from_url('http://az.lib.ru/c/chernyshewskij_n_g/text_0020.shtml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff34e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#деление текста на части (предложения), для которых будет устанавливаться авторство\n",
    "shol_sent = nltk.sent_tokenize(shol)\n",
    "klsh_sent = nltk.sent_tokenize(klsh)\n",
    "chern_sent = nltk.sent_tokenize(chern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f812e271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22348\n",
      "13629\n",
      "11002\n"
     ]
    }
   ],
   "source": [
    "#просмотр количества предложений в каждом тексте\n",
    "print(len(shol_sent))\n",
    "print(len(klsh_sent))\n",
    "print(len(chern_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ed9d780",
   "metadata": {},
   "outputs": [],
   "source": [
    "#создание датасетов\n",
    "s1 = pd.DataFrame(data=shol_sent, columns=['text'])\n",
    "k1 = pd.DataFrame(data=klsh_sent, columns=['text'])\n",
    "c1 = pd.DataFrame(data=chern_sent, columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37a9fc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#добавление столбца с авторами\n",
    "s1['author'] = ['SHOLOHOW']*len(shol_sent)\n",
    "k1['author'] = ['KALASHNIKOW']*len(klsh_sent)\n",
    "c1['author'] = ['CHERNYSHEWSKY']*len(chern_sent)\n",
    "#объединение одинаковых частей (выбор случайных 10000 строк) трех датасетов разных авторов в один\n",
    "df = pd.concat([s1.sample(10000), k1.sample(10000),c1.sample(10000)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c827fc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#перемешивание\n",
    "df = df.sample(30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f03b42b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14229</th>\n",
       "      <td>- Нет, все одно...  Вот  что  я  хотела  сказа...</td>\n",
       "      <td>SHOLOHOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6258</th>\n",
       "      <td>Кто должен был первый заметить это?</td>\n",
       "      <td>CHERNYSHEWSKY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9116</th>\n",
       "      <td>- с волнением спросила Вера Павловна.</td>\n",
       "      <td>CHERNYSHEWSKY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14473</th>\n",
       "      <td>Бесцельно бродя по двору, он осмотрел скотиний...</td>\n",
       "      <td>SHOLOHOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18086</th>\n",
       "      <td>Со  степи  набегал\\nветерок.</td>\n",
       "      <td>SHOLOHOW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text         author\n",
       "14229  - Нет, все одно...  Вот  что  я  хотела  сказа...       SHOLOHOW\n",
       "6258                 Кто должен был первый заметить это?  CHERNYSHEWSKY\n",
       "9116               - с волнением спросила Вера Павловна.  CHERNYSHEWSKY\n",
       "14473  Бесцельно бродя по двору, он осмотрел скотиний...       SHOLOHOW\n",
       "18086                       Со  степи  набегал\\nветерок.       SHOLOHOW"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#просмотр нескольких строк датасета\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b154c13",
   "metadata": {},
   "source": [
    "Перед обучением модели нужно подготовить датасет с предложениями: привести всё к одному регистру, очистить от знаков препинания и стоп-слов, удалить короткие предложения, произвести леммантизацию (привести слова к нормальной форме)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e44a3625",
   "metadata": {},
   "outputs": [],
   "source": [
    "#вычисление длины строки и добавление её в новый столбец датасета\n",
    "df['len_t'] = df.apply(lambda x: len(x['text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f822f6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>len_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10009</th>\n",
       "      <td>Эрмитаж).</td>\n",
       "      <td>CHERNYSHEWSKY</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3310</th>\n",
       "      <td>Нет, идет.</td>\n",
       "      <td>KALASHNIKOW</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17824</th>\n",
       "      <td>То-то и оно!</td>\n",
       "      <td>SHOLOHOW</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1926</th>\n",
       "      <td>- Тревога!..</td>\n",
       "      <td>SHOLOHOW</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>О, грязь!</td>\n",
       "      <td>CHERNYSHEWSKY</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               text         author  len_t\n",
       "10009     Эрмитаж).  CHERNYSHEWSKY      9\n",
       "3310     Нет, идет.    KALASHNIKOW     10\n",
       "17824  То-то и оно!       SHOLOHOW     12\n",
       "1926   - Тревога!..       SHOLOHOW     12\n",
       "976       О, грязь!  CHERNYSHEWSKY      9"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#отсечение коротких предложений (чтобы не создавались шумы), их просмотр \n",
    "df[df.len_t<15].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ce7cab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#разбиение на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.text, df.author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69ff67ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#создание экземпляра класса MorphAnalyzer\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ecbc28d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#создание списка, который содержит символы пунктуации и русские стоп-слова\n",
    "stop_w = list(punctuation) + nltk.corpus.stopwords.words('russian')\n",
    "#функция токенизации текста (приведения текста в нижний регистр, очистки от стоп-слов и регулярных выражений,\n",
    "#и приведения в нормальную форму)\n",
    "def my_tokenizer(text):\n",
    "    tokens = [morph.parse(w)[0].normal_form for w in nltk.word_tokenize(text.lower())\n",
    "             if (w not in stop_w)\n",
    "             and not re.match('.*[.`*/0-9a-z]', w)]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e84a7839",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cоздание векторизатора текста\n",
    "#для токенизации используется пользовательская функция my_tokenizer\n",
    "#создаются униграммы (токены из одного слова)\n",
    "#все признаки (токены) будут учтены без ограничений на количество\n",
    "count_v = CountVectorizer(tokenizer=my_tokenizer, ngram_range=(1,1), max_features=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1762f0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tat0e\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#создание мешка слов\n",
    "X_c = count_v.fit_transform(df.text)\n",
    "#векторизация обучающей и тестовой выборок\n",
    "X_train_c = count_v.transform(X_train)\n",
    "X_test_c = count_v.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd4b075e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22661"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#просмотр длины словаря\n",
    "len(count_v.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22ac7dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "CHERNYSHEWSKY       0.81      0.83      0.82      2440\n",
      "  KALASHNIKOW       0.82      0.79      0.81      2541\n",
      "     SHOLOHOW       0.78      0.79      0.78      2519\n",
      "\n",
      "     accuracy                           0.80      7500\n",
      "    macro avg       0.80      0.80      0.80      7500\n",
      " weighted avg       0.80      0.80      0.80      7500\n",
      " \n",
      "\n",
      "Cross-validation:  0.7999999999999999\n"
     ]
    }
   ],
   "source": [
    "#cоздание модели логистической регрессии с параметрами C=1, max_iter=200 (для сходимости)\n",
    "log_reg = LogisticRegression(C=1, max_iter=200)\n",
    "#обучение модели на подготовленных данных\n",
    "log_reg.fit(X_train_c, y_train)\n",
    "#результат классификации\n",
    "print(classification_report(y_test, log_reg.predict(X_test_c)), '\\n\\nCross-validation: ', \n",
    "      np.mean(cross_val_score(log_reg, X_c, df.author, cv = 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435c5d96",
   "metadata": {},
   "source": [
    "При использовании мешка слов для кодирования текста лучшая точность составила 0.7999999999999999, при отбрасывании предложений длины меннее 15 символов, кодированиии униграмм и параметрах регрессии: C=1, max_iter=200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47621039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tat0e\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#кодироваие с помощью TF-IDF \n",
    "#для токенизации используется специальная функция токенизатора .\n",
    "#рассматриваются как униграммы, так и биграммы.\n",
    "#сохраняются 50 000 лучших функций (на основе оценок TF-IDF).\n",
    "tfidf = TfidfVectorizer(tokenizer=my_tokenizer, ngram_range=(1,2), max_features=50000)\n",
    "#преобразование текстовых данныч в матрицы функций TF-IDF\n",
    "X_t = tfidf.fit_transform(df.text)\n",
    "X_train_t = tfidf.transform(X_train)\n",
    "X_test_t = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc6348f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "CHERNYSHEWSKY       0.79      0.89      0.83      2440\n",
      "  KALASHNIKOW       0.83      0.79      0.81      2541\n",
      "     SHOLOHOW       0.83      0.78      0.80      2519\n",
      "\n",
      "     accuracy                           0.82      7500\n",
      "    macro avg       0.82      0.82      0.82      7500\n",
      " weighted avg       0.82      0.82      0.82      7500\n",
      " \n",
      "\n",
      "Cross-validation:  0.8128666666666666\n"
     ]
    }
   ],
   "source": [
    "#cоздание модели логистической регрессии с параметрами C=10, max_iter=500 (для сходимости)\n",
    "log_reg = LogisticRegression(C=10, max_iter=500)\n",
    "#обучение модели на подготовленных данных\n",
    "log_reg.fit(X_train_t, y_train)\n",
    "#результат классификации\n",
    "print(classification_report(y_test, log_reg.predict(X_test_t)), '\\n\\nCross-validation: ', \n",
    "      np.mean(cross_val_score(log_reg, X_t, df.author, cv = 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2393b02",
   "metadata": {},
   "source": [
    "При использовании TF-IDF для кодирования текста лучшая точность составила 0.8128666666666666, при отбрасывании предложений длины меннее 15 символов, кодированиии униграмм и биграмм, размере словаря 50000 и параметрах регрессии: C=10, max_iter=500.\n",
    "Результат лучше чем при кодировании с помощью мешка слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c20d0eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#импортируем Doc2Vec и TaggedDocument\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "297d56df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#разбиение на тренировочную и тестовую выборки\n",
    "train, test = train_test_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42896b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#создание документов с токенизированными текстами и метками авторов\n",
    "train_list = train.apply(lambda x: TaggedDocument(my_tokenizer(x['text']), x['author']), axis=1)\n",
    "test_list = test.apply(lambda x: TaggedDocument(my_tokenizer(x['text']), x['author']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "168f9274",
   "metadata": {},
   "outputs": [],
   "source": [
    "#сздание модели Doc2Vec с параметрами vector_size=150, dm=1, window=3, epochs=50\n",
    "model = Doc2Vec(vector_size=150, dm=1, window=3, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a3d81d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#построение словаря\n",
    "model.build_vocab(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea206af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#обучение модели\n",
    "model.train(train_list, total_examples=model.corpus_count, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54c2e23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#фуекция преобразования текста в векторы\n",
    "def make_vector_set(model, text_list):\n",
    "    X = []\n",
    "    y = []\n",
    "    for doc in text_list:\n",
    "        X.append(model.infer_vector(doc.words))\n",
    "        y.append(doc.tags)\n",
    "    return np.array(X), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "157f8469",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование обучающего набора в векторы\n",
    "X1_train, y1_train = make_vector_set(model, train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aea1cd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразование тестового набора в векторы\n",
    "X1_test, y1_test = make_vector_set(model, test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "72b821a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "CHERNYSHEWSKY       0.70      0.76      0.73      2467\n",
      "  KALASHNIKOW       0.74      0.70      0.72      2536\n",
      "     SHOLOHOW       0.72      0.71      0.72      2497\n",
      "\n",
      "     accuracy                           0.72      7500\n",
      "    macro avg       0.72      0.72      0.72      7500\n",
      " weighted avg       0.72      0.72      0.72      7500\n",
      " \n",
      "\n",
      "Cross-validation:  0.7188\n"
     ]
    }
   ],
   "source": [
    "#cоздание модели логистической регрессии с параметрами C=1, max_iter=200 (для сходимости)\n",
    "log_reg = LogisticRegression(C=1, max_iter=500)\n",
    "#обучение модели на подготовленных данных\n",
    "log_reg.fit(X1_train, y1_train)\n",
    "#результат классификации\n",
    "print(classification_report(y1_test, log_reg.predict(X1_test)), '\\n\\nCross-validation: ', \n",
    "      np.mean(cross_val_score(log_reg, X1_test, y1_test, cv = 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b2c30b",
   "metadata": {},
   "source": [
    "При использовании модели Doc2Vec для кодирования текста лучшая точность составила 0.7188, при отбрасывании предложений длины меннее 15 символов, размере вектора 150, размере окна 3 и количестве эпох обучения - 50 и параметрах регрессии: C=1, max_iter=500.\n",
    "Результат получился хуже, чем при кодировании с помощью мешка слов и TF-IDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7141c8d3",
   "metadata": {},
   "source": [
    "Лучший результат показал способ кодирования TF-IDF, точность которого составила 0.8128666666666666, при отбрасывании предложений длины меннее 15 символов, кодированиии униграмм и биграмм, размере словаря 50000 и параметрах регрессии: C=10, max_iter=500.\n",
    "Это можно объяснить тем, что TF-IDF учитывает не только наличие слов в документе, но и их важность в контексте всего корпуса текстов. Это позволяет выделить ключевые слова и снизить влияние часто встречающихся общеупотребительных слов.\n",
    "TF-IDF может работать хорошо даже на небольших объемах данных. TF-IDF обычно менее склонен к переобучению по сравнению с более сложными моделями, такими как Doc2Vec."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
